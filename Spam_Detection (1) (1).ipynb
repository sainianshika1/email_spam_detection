{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4808,
     "status": "ok",
     "timestamp": 1561108791638,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "1RJiUokjzxRH",
    "outputId": "ac98ec52-a108-47da-cdfc-ca3c97fe9305"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-plot in c:\\users\\sompu\\anaconda3\\lib\\site-packages (0.3.7)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in c:\\users\\sompu\\anaconda3\\lib\\site-packages (from scikit-plot) (3.10.0)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in c:\\users\\sompu\\anaconda3\\lib\\site-packages (from scikit-plot) (1.6.1)\n",
      "Requirement already satisfied: scipy>=0.9 in c:\\users\\sompu\\anaconda3\\lib\\site-packages (from scikit-plot) (1.15.3)\n",
      "Requirement already satisfied: joblib>=0.10 in c:\\users\\sompu\\anaconda3\\lib\\site-packages (from scikit-plot) (1.4.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sompu\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sompu\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sompu\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sompu\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\sompu\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sompu\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\sompu\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sompu\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\sompu\\anaconda3\\lib\\site-packages (from matplotlib>=1.4.0->scikit-plot) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sompu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.17.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sompu\\anaconda3\\lib\\site-packages (from scikit-learn>=0.18->scikit-plot) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dCajKkbhHWAr"
   },
   "source": [
    "# **IMPORTING PACKAGES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wPVA7pEMxB5Q"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ehTm-dP_KGB0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import scipy\n",
    "scipy.interp = np.interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ylc3mrYeKJ7M"
   },
   "outputs": [],
   "source": [
    "#import methods\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5480,
     "status": "ok",
     "timestamp": 1561108792379,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "Df_VtO9uxB5T",
    "outputId": "313ce4bd-9b91-42ae-c5b4-ae69c39f4464"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sompu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sompu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t91dVyKrxB5a"
   },
   "source": [
    "# Read Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5836,
     "status": "ok",
     "timestamp": 1561108792777,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "-kdD0VpA4A1u",
    "outputId": "34afb10d-9bc0-489a-9fe4-6d36b994607f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('emails.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MV0omk2mxB5c"
   },
   "source": [
    "# Show Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5798,
     "status": "ok",
     "timestamp": 1561108792778,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "Jri9vTY2xB5c",
    "outputId": "2368e441-9110-49b5-fc30-746c25094f83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  spam\n",
      "0   Subject: naturally irresistible your corporate...     1\n",
      "1   Subject: the stock trading gunslinger  fanny i...     1\n",
      "2   Subject: unbelievable new homes made easy  im ...     1\n",
      "3   Subject: 4 color printing special  request add...     1\n",
      "4   Subject: do not have money , get software cds ...     1\n",
      "5   Subject: great nnews  hello , welcome to medzo...     1\n",
      "6   Subject: here ' s a hot play in motion  homela...     1\n",
      "7   Subject: save your money buy getting this thin...     1\n",
      "8   Subject: undeliverable : home based business f...     1\n",
      "9   Subject: save your money buy getting this thin...     1\n",
      "10  Subject: las vegas high rise boom  las vegas i...     1\n",
      "(5728, 2)\n"
     ]
    }
   ],
   "source": [
    "print(df.head(11))\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5791,
     "status": "ok",
     "timestamp": 1561108792779,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "21F263DfGFtC",
    "outputId": "679c7926-292e-4f8d-ca81-9feb90f4496e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5695, 2)\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5780,
     "status": "ok",
     "timestamp": 1561108792779,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "EYP7QJIRxB5f",
    "outputId": "c2634c08-69c4-4712-96fc-4d75de0e070d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  Subject: naturally irresistible your corporate...      1\n",
       "1  Subject: the stock trading gunslinger  fanny i...      1\n",
       "2  Subject: unbelievable new homes made easy  im ...      1\n",
       "3  Subject: 4 color printing special  request add...      1\n",
       "4  Subject: do not have money , get software cds ...      1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns = ['Text', 'Label']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "41kSz1ycxB5h"
   },
   "source": [
    "# Splitting the labels and the data separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5772,
     "status": "ok",
     "timestamp": 1561108792780,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "3EVkaJisxB5h",
    "outputId": "a02357ab-6d83-4a06-f1c3-2539373ac367"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1\n",
       "1     1\n",
       "2     1\n",
       "3     1\n",
       "4     1\n",
       "5     1\n",
       "6     1\n",
       "7     1\n",
       "8     1\n",
       "9     1\n",
       "10    1\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labels = df['Label']\n",
    "df_labels.head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LzlvpOdf4yFI"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "df['Text']=df['Text'].map(lambda Text: Text[6:])\n",
    "df['Text'] = df['Text'].map(lambda text:re.sub('[^A-Za-z]+', ' ',text)).apply(lambda x: (x.lower()).split())\n",
    "ps = PorterStemmer()\n",
    "df['Text']=df['Text'].apply(lambda text_list: ' '.join(list(map(lambda word : ps.stem(word),(list(filter(lambda text:text not in set(stopwords.words('english')),text_list)))))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199467,
     "status": "ok",
     "timestamp": 1561108986495,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "Sv6jId-S6fj-",
    "outputId": "69acf92f-71d5-4404-f077-60e43d47c051"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>natur irresist corpor ident lt realli hard rec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stock trade gunsling fanni merril muzo colza a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unbeliev new home made easi im want show homeo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>color print special request addit inform click...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>money get softwar cd softwar compat great grow...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  Label\n",
       "0  natur irresist corpor ident lt realli hard rec...      1\n",
       "1  stock trade gunsling fanni merril muzo colza a...      1\n",
       "2  unbeliev new home made easi im want show homeo...      1\n",
       "3  color print special request addit inform click...      1\n",
       "4  money get softwar cd softwar compat great grow...      1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QLhTP8mJxB5k"
   },
   "source": [
    "# Data Visualization\n",
    " - To check the most used word in Ham Email and Spam Email\n",
    " - To visualize the percentage of Ham and Spam Email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1265
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 439039,
     "status": "ok",
     "timestamp": 1561109226077,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "pzXAJinTxB5l",
    "outputId": "aa9270b7-675c-4b5e-e52c-37c5d40d96ab"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\sompu/nltk_data'\n    - 'C:\\\\Users\\\\sompu\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\sompu\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\sompu\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\sompu\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m words \u001b[38;5;129;01min\u001b[39;00m ham_dataset\u001b[38;5;241m.\u001b[39mText:\n\u001b[0;32m     12\u001b[0m     txt \u001b[38;5;241m=\u001b[39m words\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m---> 13\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(txt)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m     15\u001b[0m         ham_words \u001b[38;5;241m=\u001b[39m ham_words \u001b[38;5;241m+\u001b[39m word \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mword_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m, preserve_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;124;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m     sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    144\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    145\u001b[0m     ]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msent_tokenize\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    110\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m _get_punkt_tokenizer(language)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[0m, in \u001b[0;36m_get_punkt_tokenizer\u001b[1;34m(language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mlru_cache\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_punkt_tokenizer\u001b[39m(language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03m    a lru cache for performance.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;124;03m    :type language: str\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[0m, in \u001b[0;36mPunktTokenizer.__init__\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1743\u001b[0m     PunktSentenceTokenizer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1744\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_lang(lang)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[0m, in \u001b[0;36mPunktTokenizer.load_lang\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   1747\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[1;32m-> 1749\u001b[0m     lang_dir \u001b[38;5;241m=\u001b[39m find(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_params \u001b[38;5;241m=\u001b[39m load_punkt_params(lang_dir)\n\u001b[0;32m   1751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang \u001b[38;5;241m=\u001b[39m lang\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\sompu/nltk_data'\n    - 'C:\\\\Users\\\\sompu\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\sompu\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\sompu\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\sompu\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "stopwords = STOPWORDS\n",
    "stopwords = list(stopwords)\n",
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "stopwords = stopwords + STOPWORDS\n",
    "\n",
    "ham_dataset = df[df.Label == 0]\n",
    "spam_dataset = df[df.Label == 1]\n",
    "ham_words = ' '\n",
    "spam_words = ' '\n",
    "\n",
    "for words in ham_dataset.Text:\n",
    "    txt = words.lower()\n",
    "    tokens = nltk.word_tokenize(txt)\n",
    "    for word in tokens:\n",
    "        ham_words = ham_words + word + \" \"\n",
    "for words in spam_dataset.Text:\n",
    "    txt = words.lower()\n",
    "    tokens = nltk.word_tokenize(txt)\n",
    "    for word in tokens:\n",
    "        spam_words = spam_words + word + \" \"\n",
    "\n",
    "def gen_wordcloud(wordcloud):\n",
    "    plt.figure(figsize = (10,8))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "print(\"\\n\")    \n",
    "print(\"\\t\\t\\t\\t HAM WORDS\")    \n",
    "wordcloud = WordCloud(background_color = 'white', width = 500, height = 500, stopwords = stopwords,\n",
    "                     max_words = 500, max_font_size = 50, random_state = 42).generate(ham_words)\n",
    "gen_wordcloud(wordcloud)\n",
    "\n",
    "print(\"\\t\\t\\t\\t SPAM WORDS\")\n",
    "wordcloud = WordCloud(background_color = 'white', width = 500, height = 500, stopwords = stopwords,\n",
    "                     max_words = 500, max_font_size = 50, random_state = 42).generate(spam_words)\n",
    "gen_wordcloud(wordcloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2ENHRTGxB5q"
   },
   "source": [
    "## Plotting ham and spam data % in pie chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 439042,
     "status": "ok",
     "timestamp": 1561109226088,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "L6-xvHNRxB5q",
    "outputId": "78bb684f-f316-4273-f9be-1f1da3e2200f"
   },
   "outputs": [],
   "source": [
    "count_Class = pd.value_counts(df.Label, sort = True)\n",
    "print(count_Class)\n",
    "# Data to Plot\n",
    "labels = 'Ham', 'Spam'\n",
    "sizes = [count_Class[0], count_Class[1]]\n",
    "colors = ['lightskyblue', 'aqua']\n",
    "explode = (0.1, 0.1)\n",
    "\n",
    "# Plot\n",
    "plt.pie(sizes, explode = explode, labels = labels, colors = colors,\n",
    "        autopct = '%1.1f%%', shadow = True, startangle = 90)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g73sIidhxB5s"
   },
   "source": [
    "# Splitting the Test and Train Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 439032,
     "status": "ok",
     "timestamp": 1561109226090,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "RqbDzXDUxB5t",
    "outputId": "41bd5cb8-eb72-4e59-ff0c-75248613c6d5"
   },
   "outputs": [],
   "source": [
    "train_set, test_set, train_label, test_label = train_test_split(df, df_labels, test_size = 0.33, random_state = 42)\n",
    "print(train_set.shape)\n",
    "print(test_set.shape)\n",
    "print(\"\\nThe Trainset consists of {} records and {} features\".format(train_set.shape[0],train_set.shape[1]))\n",
    "print(\"\\nThe Testset consists of {} records and {} features\".format(test_set.shape[0],train_set.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qcZYOLXhxB5v"
   },
   "source": [
    "# Extracting N-grams from the Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0yAwo5y4xB5w"
   },
   "outputs": [],
   "source": [
    "countvect = CountVectorizer(ngram_range = (2,2), )\n",
    "x_counts = countvect.fit(train_set.Text)\n",
    "\n",
    "# preparing for training set\n",
    "x_train_df = countvect.transform(train_set.Text)\n",
    "\n",
    "# preparing for test set\n",
    "x_test_df = countvect.transform(test_set.Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nPoaHRGmxB5y"
   },
   "source": [
    "# Data Model\n",
    "\n",
    "The Algorithms used below in this notebooks are:\n",
    "\n",
    " - Naive Bayes\n",
    " - K-Nearest\n",
    " - Decision Tree\n",
    " - Support Vector Machine\n",
    " - Random Forest\n",
    " -MLP Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z9wDWrk1xB5y"
   },
   "source": [
    "# Naive Bayes classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 440616,
     "status": "ok",
     "timestamp": 1561109227695,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "QdnZCHwaxB5z",
    "outputId": "77537385-e5f4-4a89-e669-6aceddb77910"
   },
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(x_train_df,train_set.Label)\n",
    "predicted_values_NB = clf.predict(x_test_df)\n",
    "predictions = dict()\n",
    "accuracy = accuracy_score(test_set.Label, predicted_values_NB)\n",
    "predictions['Naive Bayes'] = accuracy * 100\n",
    "confusionmatrix = confusion_matrix(test_set.Label, predicted_values_NB)\n",
    "print(\"Accuracy of Naive Bayes classifier is {}%\".format(accuracy * 100))\n",
    "print(\"\\n\", confusionmatrix)\n",
    "skplt.metrics.plot_confusion_matrix(test_set.Label, predicted_values_NB, normalize = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8AcXg2NJxB53"
   },
   "source": [
    "# K-Nearest Neighbors algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 441604,
     "status": "ok",
     "timestamp": 1561109228693,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "2wRoK4I0xB54",
    "outputId": "3f54b712-d358-45bf-d9f5-bb5c8e8cca1c"
   },
   "outputs": [],
   "source": [
    "KNN = KNeighborsClassifier()\n",
    "KNN.fit(x_train_df, train_set.Label)\n",
    "predicted_values_KNN = KNN.predict(x_test_df)\n",
    "print(predicted_values_KNN)\n",
    "accuracy_KNN = accuracy_score(test_set.Label, predicted_values_KNN)\n",
    "predictions['K-Nearest Neighbors algorithm'] = accuracy_KNN * 100\n",
    "print(\"\\nThe accuracy of K-Nearest Neighbors algorithm is {}%\".format(accuracy_KNN * 100))\n",
    "confusion_matrix_KNN = confusion_matrix(test_set.Label, predicted_values_KNN)\n",
    "print(\"\\n\", confusion_matrix_KNN)\n",
    "skplt.metrics.plot_confusion_matrix(test_set.Label, predicted_values_KNN, normalize = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rkyxYpSxxB57"
   },
   "source": [
    "# Decision Tree learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 448642,
     "status": "ok",
     "timestamp": 1561109235739,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "mvKsGKzJxB57",
    "outputId": "9525cdbf-1edf-454b-a285-bf9a90d58b97"
   },
   "outputs": [],
   "source": [
    "DT = DecisionTreeClassifier()\n",
    "DT.fit(x_train_df, train_set.Label)\n",
    "predicted_values_DT = DT.predict(x_test_df)\n",
    "print(predicted_values_DT)\n",
    "accuracy_DT = accuracy_score(test_set.Label, predicted_values_DT)\n",
    "predictions['Decision Tree learning'] = accuracy_DT * 100\n",
    "print(\"\\nThe accuracy of Decision Tree learning is {}%\".format(accuracy_DT * 100))\n",
    "confusion_matrix_DT = confusion_matrix(test_set.Label, predicted_values_DT)\n",
    "print(\"\\n\", confusion_matrix_DT)\n",
    "skplt.metrics.plot_confusion_matrix(test_set.Label, predicted_values_DT, normalize = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0IDMmlQIxB5_"
   },
   "source": [
    "# Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 458119,
     "status": "ok",
     "timestamp": 1561109245222,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "MbmO1Rf2xB6B",
    "outputId": "98896c70-e6a2-45fb-fa6e-b3a7e64660a0"
   },
   "outputs": [],
   "source": [
    "SVM = svm.SVC()\n",
    "SVM.fit(x_train_df, train_set.Label)\n",
    "predicted_values_SVM = SVM.predict(x_test_df)\n",
    "print(predicted_values_SVM)\n",
    "accuracy_SVM = accuracy_score(test_set.Label, predicted_values_SVM)\n",
    "predictions['Support Vector Machine (SVM)'] = accuracy_SVM * 100\n",
    "print(\"\\nThe accuracy of Support Vector Machine (SVM) is {}%\".format(accuracy_SVM * 100))\n",
    "confusion_matrix_SVM = confusion_matrix(test_set.Label, predicted_values_SVM)\n",
    "print(\"\\n\", confusion_matrix_SVM)\n",
    "skplt.metrics.plot_confusion_matrix(test_set.Label, predicted_values_SVM, normalize = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DmsuSkNYxB6G"
   },
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 397
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 469742,
     "status": "ok",
     "timestamp": 1561109256855,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "E4FbCWlzxB6H",
    "outputId": "a3a65274-c05b-45a5-879c-bf1b21c0ea6c"
   },
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators = 100, oob_score = True, random_state = 123456)\n",
    "\n",
    "RF.fit(x_train_df, train_set.Label)\n",
    "predicted_values_RF = RF.predict(x_test_df)\n",
    "print(predicted_values_RF)\n",
    "accuracy_RF = accuracy_score(test_set.Label, predicted_values_RF)\n",
    "predictions['Random Forest'] = accuracy_RF * 100\n",
    "print(\"\\nThe accuracy of Random Forest is {}%\".format(accuracy_RF * 100))\n",
    "confusion_matrix_RF = confusion_matrix(test_set.Label, predicted_values_RF)\n",
    "print(\"\\n\", confusion_matrix_RF)\n",
    "skplt.metrics.plot_confusion_matrix(test_set.Label, predicted_values_RF, normalize = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vqg4d7IunKfy"
   },
   "source": [
    "# **MLP Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 469736,
     "status": "ok",
     "timestamp": 1561109256856,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "KlEUw-HCnRCc",
    "outputId": "fa1fb43c-ec1f-47da-b701-c0cf4457b8c1"
   },
   "outputs": [],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes = (6,2), activation = 'relu',learning_rate = 'adaptive', max_iter = 100)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 581483,
     "status": "ok",
     "timestamp": 1561109368610,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "1zekewuFnQjZ",
    "outputId": "8e835873-c721-4a6f-f658-f64e7c72d22b"
   },
   "outputs": [],
   "source": [
    "clf.fit(x_train_df,train_set.Label)\n",
    "pred=clf.predict(x_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 581476,
     "status": "ok",
     "timestamp": 1561109368612,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "ZscCqZV9nd8U",
    "outputId": "e639bd35-3c7a-4764-d8f1-e774c64c3f06"
   },
   "outputs": [],
   "source": [
    "predictions['MLPclassifier'] = accuracy_RF * 100\n",
    "print(\"\\nThe accuracy of MLPclassifier is {}%\".format(accuracy_RF * 100))\n",
    "confusion_matrix_RF = confusion_matrix(test_set.Label, pred)\n",
    "print(\"\\n\", confusion_matrix_RF)\n",
    "skplt.metrics.plot_confusion_matrix(test_set.Label, pred, normalize = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PUMOiwmMxB6L"
   },
   "source": [
    "# Method Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 582122,
     "status": "ok",
     "timestamp": 1561109369264,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "986SAjeAxB6L",
    "outputId": "2c17497d-02c7-49db-91f0-a1666298fd1a"
   },
   "outputs": [],
   "source": [
    "fig, (ax1) = plt.subplots(ncols = 1, sharey = True,figsize = (15,5))\n",
    "df1 = pd.DataFrame(list(predictions.items()),columns = ['Algorithms','Percentage'])\n",
    "display(df1)\n",
    "sns.pointplot(x = \"Algorithms\", y = \"Percentage\", data = df1,ax = ax1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wSBn7P2pxB6N"
   },
   "source": [
    "# ROC Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 582117,
     "status": "ok",
     "timestamp": 1561109369266,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "fB5p4oRwxB6N",
    "outputId": "00337fe4-e8ea-4c3e-cdc1-d9d345a2c5fe"
   },
   "outputs": [],
   "source": [
    "test_prediction = test_set.Label.tolist()\n",
    "predicted_values = predicted_values_NB.tolist()\n",
    "test_prediction = [1 if pred==1 else 0 for pred in test_prediction]\n",
    "predicted_values = [1 if pred==1 else 0 for pred in predicted_values]\n",
    "fpr, tpr, thresholds = roc_curve(test_prediction,predicted_values)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "print(\"The ROC Accuracy is {}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 582842,
     "status": "ok",
     "timestamp": 1561109369998,
     "user": {
      "displayName": "Venkat Sai",
      "photoUrl": "https://lh6.googleusercontent.com/-aOiCOY-r2_o/AAAAAAAAAAI/AAAAAAAAD-I/rEM7OdDZZtM/s64/photo.jpg",
      "userId": "01452027474227760656"
     },
     "user_tz": -330
    },
    "id": "EBPpVe9-xB6Q",
    "outputId": "6450f07c-3a4f-4bf9-b57c-270384ec85f6"
   },
   "outputs": [],
   "source": [
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b',\n",
    "label='AUC = %0.2f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Spam_Detection.ipynb",
   "provenance": [
    {
     "file_id": "1fv3Pg5nxUV4dftY-ngmzmdXXWwlU_dMI",
     "timestamp": 1560830917537
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
